# LLM Function Calling æŠ€æœ¯æ–¹æ¡ˆæ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†é˜è¿°äº†åŸºäºOpenAI APIçš„LLM Function CallingæŠ€æœ¯å®ç°æ–¹æ¡ˆï¼ŒåŒ…å«ä¸¤ç§å®ç°æ¨¡å¼ï¼šç®€å•å‡½æ•°è°ƒç”¨(Simple Function Call)å’ŒLangChainæ€ç»´é“¾(CoT)å‡½æ•°è°ƒç”¨ã€‚é¡¹ç›®é€šè¿‡å®šä¹‰æ ‡å‡†å·¥å…·å‡½æ•°æ¥å£ï¼Œå®ç°LLMä¸å¤–éƒ¨å·¥å…·çš„åŠ¨æ€äº¤äº’ã€‚

## æŠ€æœ¯æ¶æ„

### ç³»ç»Ÿæ¶æ„å›¾

```mermaid
graph TB
    subgraph "å®¢æˆ·ç«¯å±‚"
        A[ç”¨æˆ·è¾“å…¥] --> B[LLMè°ƒç”¨å™¨]
        B --> C[å‡½æ•°è§£æå™¨]
    end
    
    subgraph "å‡½æ•°ç®¡ç†å±‚"
        C --> D{å‡½æ•°é€‰æ‹©å™¨}
        D --> E[å·¥å…·å‡½æ•°åº“]
        E --> F[æ•°å­¦è®¡ç®—]
        E --> G[å¥åº·æŠ¥å‘Š]
        E --> H[è¿åŠ¨æŠ¥å‘Š]
        E --> I[è¿åŠ¨åˆ†æ]
    end
    
    subgraph "æ•°æ®å±‚"
        G --> J[health_report.md]
        H --> K[sport_tracing.md]
        I --> L[LLMåˆ†æå¼•æ“]
    end
    
    F --> M[è¿”å›ç»“æœ]
    G --> M
    H --> M
    I --> M
    M --> N[æœ€ç»ˆè¾“å‡º]
```

### æ ¸å¿ƒç»„ä»¶æ¶æ„

```mermaid
classDiagram
    class LLMClient {
        -base_url: string
        -api_key: string
        -model_name: string
        +chat_completions_create()
    }
    
    class ToolFunction {
        <<interface>>
        +name: string
        +description: string
        +parameters: dict
        +execute()
    }
    
    class SimpleFunctionCall {
        -tools: list
        -messages: list
        +llm_call()
        +parse_llm_response()
    }
    
    class LangChainCoT {
        -agent_executor: AgentExecutor
        -memory: ConversationBufferMemory
        +create_agent_executor()
        +process_with_langchain()
    }
    
    class MathTools {
        +solve(symbols, equation)
        +multiply(multiplicand, multiplier)
    }
    
    class HealthTools {
        +my_health_report()
        +my_sport_report()
        +analyze_sport_report(report)
    }
    
    LLMClient --> SimpleFunctionCall
    LLMClient --> LangChainCoT
    SimpleFunctionCall --> ToolFunction
    LangChainCoT --> ToolFunction
    ToolFunction <|-- MathTools
    ToolFunction <|-- HealthTools
```

## å®ç°æ–¹æ¡ˆè¯¦è§£

### æ–¹æ¡ˆä¸€ï¼šç®€å•å‡½æ•°è°ƒç”¨å®ç°

#### 1.1 å·¥å…·å®šä¹‰è§„èŒƒ

```python
# å·¥å…·å®šä¹‰æ ‡å‡†æ ¼å¼
tools = [
    {
        "type": "function",
        "function": {
            "name": "solve",
            "description": "æ±‚è§£æ–¹ç¨‹",
            "parameters": {
                "type": "object",
                "properties": {
                    "symbols": {"type": "string", "description": "ç¬¦å·"},
                    "equation": {"type": "string", "description": "æ–¹ç¨‹"}
                },
                "required": ["symbols", "equation"]
            }
        }
    }
]
```

#### 1.2 è°ƒç”¨æµç¨‹æ—¶åºå›¾

```mermaid
sequenceDiagram
    participant User
    participant LLMCall
    participant OpenAI_API
    participant Parser
    participant Tool
    
    User->>LLMCall: å‘é€é—®é¢˜
    LLMCall->>OpenAI_API: è°ƒç”¨API(å«tools)
    OpenAI_API-->>LLMCall: è¿”å›å‡½æ•°è°ƒç”¨å»ºè®®
    LLMCall->>Parser: è§£æå‡½æ•°è°ƒç”¨
    Parser->>Tool: æ‰§è¡Œå…·ä½“å‡½æ•°
    Tool-->>Parser: è¿”å›æ‰§è¡Œç»“æœ
    Parser->>OpenAI_API: å†æ¬¡è°ƒç”¨API(å«ç»“æœ)
    OpenAI_API-->>LLMCall: è¿”å›æœ€ç»ˆç­”æ¡ˆ
    LLMCall-->>User: å±•ç¤ºç»“æœ
```

#### 1.3 æ ¸å¿ƒä»£ç å®ç°

```python
def parse_llm_response(model_response, messages):
    """
    è§£æLLMå“åº”å¹¶å¤„ç†å‡½æ•°è°ƒç”¨
    å…³é”®ç‰¹æ€§ï¼š
    1. é€’å½’å¤„ç†ç›´åˆ°æ— å‡½æ•°è°ƒç”¨
    2. å‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†
    3. ç»“æœä¼ é€’å’Œä¸Šä¸‹æ–‡ç»´æŠ¤
    """
    
    while model_response.choices[0].message.tool_calls:
        tool_call = model_response.choices[0].message.tool_calls[0]
        args = tool_call.function.arguments
        
        # å‡½æ•°è·¯ç”±åˆ†å‘
        if tool_call.function.name == "solve":
            function_result = solve(**json.loads(args))
        elif tool_call.function.name == "multiply":
            function_result = multiply(**json.loads(args))
        elif tool_call.function.name == "analyze_sport_report":
            # å‚æ•°éªŒè¯å’Œé”™è¯¯å¤„ç†
            try:
                args_dict = json.loads(args)
                report = args_dict.get('report')
                if report is None:
                    report = my_sport_report()  # å¤‡ç”¨æ–¹æ¡ˆ
                function_result = analyze_sport_report(report)
            except Exception as e:
                function_result = {"error": str(e)}
        
        # æ„å»ºå·¥å…·è°ƒç”¨æ¶ˆæ¯
        messages.append({
            "role": "tool",
            "content": json.dumps(function_result),
            "tool_call_id": tool_call.id
        })
        
        # ç»§ç»­ä¸‹ä¸€è½®è°ƒç”¨
        model_response = client.chat.completions.create(
            model=model_name,
            messages=messages,
            tools=tools
        )
    
    return model_response.choices[0].message.content
```

### æ–¹æ¡ˆäºŒï¼šLangChainæ€ç»´é“¾(CoT)å®ç°

#### 2.1 æ¶æ„è®¾è®¡

```mermaid
graph LR
    A[ç”¨æˆ·è¾“å…¥] --> B[AgentExecutor]
    B --> C[Tool Calling Agent]
    C --> D[LLMæ¨ç†]
    D --> E[å·¥å…·é€‰æ‹©]
    E --> F[å‡½æ•°æ‰§è¡Œ]
    F --> G[ç»“æœè¿”å›]
    G --> H[è®°å¿†å­˜å‚¨]
    H --> C
```

#### 2.2 Agenté…ç½®è¯¦è§£

```python
def create_agent_executor() -> AgentExecutor:
    """
    åˆ›å»ºLangChain Agentæ‰§è¡Œå™¨
    æ ¸å¿ƒé…ç½®ï¼š
    1. å¼ºåˆ¶å·¥å…·è°ƒç”¨ç­–ç•¥
    2. å¯¹è¯è®°å¿†ç®¡ç†
    3. é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
    """
    
    # LLMé…ç½®
    llm = ChatOpenAI(
        base_url="http://localhost:11434/v1",
        api_key="ollama",
        model=model_name,
        temperature=0.1  # é™ä½éšæœºæ€§ï¼Œç¡®ä¿ä¸€è‡´æ€§
    )
    
    # å·¥å…·å®šä¹‰
    tools = create_langchain_tools()
    
    # å¼ºåŒ–æç¤ºæ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", 
         "ä½ æ˜¯ä¸€ä¸ª**å¿…é¡»ä¸¥æ ¼éµå®ˆæŒ‡ä»¤**çš„æ™ºèƒ½åŠ©æ‰‹ã€‚\n"
         "å½“ç”¨æˆ·è¦æ±‚åˆ†æè¿åŠ¨æŠ¥å‘Šæ—¶ï¼Œå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ‰§è¡Œï¼š\n"
         "æ­¥éª¤1ï¼šç«‹å³è°ƒç”¨my_sport_reportè·å–æŠ¥å‘Š\n"
         "æ­¥éª¤2ï¼šå°†å®Œæ•´æŠ¥å‘Šä¼ é€’ç»™analyze_sport_report\n"
         "æ­¥éª¤3ï¼šè¿”å›åˆ†æç»“æœä½œä¸ºæœ€ç»ˆç­”æ¡ˆ\n"
         "**é‡è¦**ï¼šå¿…é¡»è°ƒç”¨analyze_sport_reportï¼Œå¦åˆ™ä»»åŠ¡æœªå®Œæˆ"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad")
    ])
    
    # Agentåˆ›å»º
    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)
    
    # æ‰§è¡Œå™¨é…ç½®
    return AgentExecutor.from_agent_and_tools(
        agent=agent,
        tools=tools,
        memory=ConversationBufferMemory(),
        verbose=True,
        max_iterations=10,
        handle_parsing_errors=True,
        return_intermediate_steps=True
    )
```

#### 2.3 æ‰§è¡Œæµç¨‹ç›‘æ§

```python
def process_with_langchain(message: str) -> str:
    """
    LangChainå¤„ç†æµç¨‹
    ç›‘æ§ç‚¹ï¼š
    1. å·¥å…·è°ƒç”¨é¡ºåº
    2. å‚æ•°ä¼ é€’æ­£ç¡®æ€§
    3. å¼‚å¸¸å¤„ç†æœºåˆ¶
    4. æ‰‹åŠ¨å…œåº•ç­–ç•¥
    """
    
    agent_executor = create_agent_executor()
    
    try:
        result = agent_executor.invoke({"input": message})
        
        # å·¥å…·è°ƒç”¨éªŒè¯
        analyze_called = False
        final_analysis = ""
        
        # åˆ†æä¸­é—´æ­¥éª¤
        if "intermediate_steps" in result:
            for step in result["intermediate_steps"]:
                if step[0].tool == "analyze_sport_report":
                    analyze_called = True
        
        # æ‰‹åŠ¨å…œåº•æœºåˆ¶
        if not analyze_called:
            sport_report = None
            for step in result["intermediate_steps"]:
                if step[0].tool == "my_sport_report":
                    sport_report = step[1]
                    break
            
            if sport_report:
                final_analysis = analyze_sport_report(sport_report)
                result["output"] = final_analysis
        
        return result["output"]
        
    except Exception as e:
        return f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
```

## å·¥å…·å‡½æ•°è¯¦ç»†è®¾è®¡

### æ•°å­¦è®¡ç®—å·¥å…·

#### æ–¹ç¨‹æ±‚è§£å™¨

```python
def solve(symbols: str, equation: str) -> dict:
    """
    ä½¿ç”¨SymPyæ±‚è§£æ•°å­¦æ–¹ç¨‹
    
    Args:
        symbols: å˜é‡ç¬¦å·å®šä¹‰
        equation: æ–¹ç¨‹å­—ç¬¦ä¸²ï¼Œæ ¼å¼å¦‚ "x**2 - 4 = 0"
    
    Returns:
        dict: åŒ…å«ç¬¦å·ã€æ–¹ç¨‹å’Œè§£çš„å­—å…¸
        
    Example:
        >>> solve("x", "x**2 - 4 = 0")
        {'symbols': 'x', 'equation': 'x**2 - 4 = 0', 'solutions': '[-2, 2]'}
    """
    x = sp.symbols('x')
    _equation = sp.sympify(equation.split('=')[0])
    _equation = sp.Eq(_equation, 0)
    solutions = sp.solve(_equation, x)
    
    return {
        "symbols": symbols,
        "equation": equation,
        "solutions": str(solutions)
    }
```

#### å¤§æ•°ä¹˜æ³•å™¨

```python
def multiply(multiplicand: float, multiplier: float) -> dict:
    """
    é«˜ç²¾åº¦å¤§æ•°ä¹˜æ³•è®¡ç®—
    
    Args:
        multiplicand: è¢«ä¹˜æ•°
        multiplier: ä¹˜æ•°
    
    Returns:
        dict: åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸
    """
    result = multiplicand * multiplier
    return {"value": result}
```

### å¥åº·æ•°æ®å·¥å…·

#### å¥åº·æŠ¥å‘Šè·å–

```python
def my_health_report() -> str:
    """
    ä»æœ¬åœ°æ–‡ä»¶è¯»å–å¥åº·æŠ¥å‘Š
    
    æ•°æ®æ ¼å¼:
    - æ–‡ä»¶: health_report.md
    - å†…å®¹: Markdownæ ¼å¼çš„å¥åº·æ•°æ®
    
    Returns:
        str: å®Œæ•´çš„å¥åº·æŠ¥å‘Šæ–‡æœ¬
    """
    try:
        with open("health_report.md", "r", encoding="utf-8") as f:
            report = f.read()
        return report
    except FileNotFoundError:
        return "å¥åº·æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨"
    except Exception as e:
        return f"è¯»å–å¥åº·æŠ¥å‘Šå¤±è´¥: {str(e)}"
```

#### è¿åŠ¨æŠ¥å‘Šåˆ†æ

```python
def analyze_sport_report(report: str) -> str:
    """
    ä½¿ç”¨LLMè¿›è¡Œè¿åŠ¨æŠ¥å‘Šæ™ºèƒ½åˆ†æ
    
    åˆ†æç»´åº¦:
    1. è¿åŠ¨ç±»å‹å’Œé¢‘ç‡åˆ†æ
    2. è¿åŠ¨å¼ºåº¦è¯„ä¼°
    3. å¥åº·è¶‹åŠ¿è¯†åˆ«
    4. ä¸ªæ€§åŒ–å»ºè®®ç”Ÿæˆ
    
    Args:
        report: åŸå§‹è¿åŠ¨æŠ¥å‘Šæ–‡æœ¬
    
    Returns:
        str: AIç”Ÿæˆçš„åˆ†ææŠ¥å‘Š
    """
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¿åŠ¨åŒ»å­¦ä¸“å®¶..."},
        {"role": "user", "content": f"""
        è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ†æè¿åŠ¨æŠ¥å‘Šï¼š
        1. ç†è§£è¿åŠ¨ç±»å‹ã€æ—¶é•¿ã€é¢‘ç‡
        2. åˆ†æå…³é”®æŒ‡æ ‡ï¼ˆè¿åŠ¨é¢‘ç‡ã€å¼ºåº¦ï¼‰
        3. è¯„ä¼°è®¡åˆ’åˆç†æ€§
        4. ç»™å‡ºé’ˆå¯¹æ€§å»ºè®®
        5. ç»˜åˆ¶è¿åŠ¨è¶‹åŠ¿å›¾è¡¨
        6. æ€»ç»“è¿åŠ¨æ•ˆæœ
        
        è¿åŠ¨æŠ¥å‘Šï¼š{report}
        """}
    ]
    
    response = client.chat.completions.create(
        model=model_name,
        messages=messages,
        temperature=0.9
    )
    
    return response.choices[0].message.content
```

## é”™è¯¯å¤„ç†å’Œç›‘æ§

### å¼‚å¸¸å¤„ç†æœºåˆ¶

```mermaid
flowchart TD
    A[å‡½æ•°è°ƒç”¨] --> B{å‚æ•°éªŒè¯}
    B -->|æœ‰æ•ˆ| C[æ‰§è¡Œå‡½æ•°]
    B -->|æ— æ•ˆ| D[é”™è¯¯å¤„ç†]
    C --> E{æ‰§è¡Œç»“æœ}
    E -->|æˆåŠŸ| F[è¿”å›ç»“æœ]
    E -->|å¤±è´¥| G[é‡è¯•æœºåˆ¶]
    G --> H{é‡è¯•æ¬¡æ•°}
    H -->|<3æ¬¡| C
    H -->|>=3æ¬¡| I[è¿”å›é”™è¯¯]
    D --> J[å‚æ•°ä¿®æ­£]
    J --> C
```

### æ—¥å¿—å’Œç›‘æ§

```python
import logging
from colorama import Fore, Style

# é…ç½®å½©è‰²æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format=f'{Fore.GREEN}%(asctime)s{Style.RESET_ALL} - '
           f'{Fore.YELLOW}%(levelname)s{Style.RESET_ALL} - '
           f'{Fore.WHITE}%(message)s{Style.RESET_ALL}'
)

def log_function_call(func_name, params, result):
    """è®°å½•å‡½æ•°è°ƒç”¨æ—¥å¿—"""
    logging.info(f"ğŸ”„ è°ƒç”¨å‡½æ•°: {func_name}")
    logging.debug(f"ğŸ“¥ å‚æ•°: {params}")
    logging.info(f"ğŸ“¤ ç»“æœ: {str(result)[:100]}...")
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç¼“å­˜æœºåˆ¶

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def cached_sport_report():
    """ç¼“å­˜è¿åŠ¨æŠ¥å‘Šï¼Œé¿å…é‡å¤è¯»å–æ–‡ä»¶"""
    return my_sport_report()
```

### 2. å¹¶å‘å¤„ç†

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def batch_analyze_reports(reports):
    """æ‰¹é‡åˆ†æå¤šä¸ªè¿åŠ¨æŠ¥å‘Š"""
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(analyze_sport_report, report) 
                  for report in reports]
        return [f.result() for f in futures]
```

### 3. è¿æ¥æ± ä¼˜åŒ–

```python
from openai import AsyncOpenAI

# ä½¿ç”¨å¼‚æ­¥å®¢æˆ·ç«¯æå‡æ€§èƒ½
async_client = AsyncOpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
)
```

## æµ‹è¯•ç”¨ä¾‹

### å•å…ƒæµ‹è¯•ç¤ºä¾‹

```python
import pytest

def test_solve_equation():
    """æµ‹è¯•æ–¹ç¨‹æ±‚è§£"""
    result = solve("x", "x**2 - 4 = 0")
    assert result["solutions"] == "[-2, 2]"

def test_multiply_large_numbers():
    """æµ‹è¯•å¤§æ•°ä¹˜æ³•"""
    result = multiply(123456789, 987654321)
    assert result["value"] == 123456789 * 987654321

def test_health_report_exists():
    """æµ‹è¯•å¥åº·æŠ¥å‘Šæ–‡ä»¶å­˜åœ¨"""
    report = my_health_report()
    assert isinstance(report, str)
    assert len(report) > 0
```

## éƒ¨ç½²å’Œè¿ç»´

### ç¯å¢ƒé…ç½®

```yaml
# docker-compose.yml
version: '3.8'
services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
      - ./data:/app/data
  
  llm-app:
    build: .
    depends_on:
      - ollama
    environment:
      - OPENAI_BASE_URL=http://ollama:11434/v1
      - MODEL_NAME=qwen2
```

### ç›‘æ§æŒ‡æ ‡

- å‡½æ•°è°ƒç”¨æˆåŠŸç‡
- å¹³å‡å“åº”æ—¶é—´
- é”™è¯¯ç‡ç»Ÿè®¡
- èµ„æºä½¿ç”¨ç›‘æ§

## æ€»ç»“

æœ¬æŠ€æœ¯æ–¹æ¡ˆé€šè¿‡ä¸¤ç§å®ç°æ¨¡å¼å±•ç¤ºäº†LLM Function Callingçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼š

1. **ç®€å•æ¨¡å¼**ï¼šè½»é‡çº§ã€ç›´æ¥è°ƒç”¨ï¼Œé€‚åˆç®€å•åœºæ™¯
2. **LangChainæ¨¡å¼**ï¼šä¼ä¸šçº§ã€å¯æ‰©å±•ï¼Œæ”¯æŒå¤æ‚ä¸šåŠ¡æµç¨‹

ä¸¤ç§æ–¹æ¡ˆéƒ½å…·å¤‡ï¼š
- å®Œæ•´çš„é”™è¯¯å¤„ç†æœºåˆ¶
- è¯¦ç»†çš„æ—¥å¿—ç›‘æ§
- å¯æ‰©å±•çš„å·¥å…·ä½“ç³»
- é«˜æ€§èƒ½çš„å¹¶å‘æ”¯æŒ

å¯æ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„å®ç°æ–¹æ¡ˆã€‚